{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e246ea1-39ca-4652-8dc0-38ccfabd2f96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Script 1 - Extract Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b21486-4475-4382-8e1d-5e0463981d69",
   "metadata": {},
   "source": [
    "_Script by Tim Hebestreit, thebestr@smail.uni-koeln.de_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425706d9-bd0e-410a-b9b2-49dd5529cca8",
   "metadata": {},
   "source": [
    "In this notebook, the articles are extracted from the raw exported docx and pdf files. To replicate the export of the csv file, simply run all cells, but note that this will take 30-60 minutes depending on your machine. Also, note that this is optional as the output csv file (*articles_raw.csv*) is already present in the data/csv folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614d4ff-bb5e-4c04-b3df-554bfd6b1b7e",
   "metadata": {},
   "source": [
    "First, we install the needed packages for this script:\n",
    "\n",
    "- *pdfplumber* helps with reading in and parsing pdf files\n",
    "- *tqdm* makes the import process more clear with a smart progress bar\n",
    "- *pandas* is used to create a dataframe of the parsed articles\n",
    "- *python-docx* (or just docx) helps with the import of docx files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab173b0-3d1e-4c4f-9f1f-44a1b49079fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /opt/conda/lib/python3.11/site-packages (0.11.8)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: python-docx in /opt/conda/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in /opt/conda/lib/python3.11/site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.11/site-packages (from pdfplumber) (10.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/conda/lib/python3.11/site-packages (from pdfplumber) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from pdfminer.six==20251107->pdfplumber) (3.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.11/site-packages (from pdfminer.six==20251107->pdfplumber) (41.0.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /opt/conda/lib/python3.11/site-packages (from python-docx) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION ---\n",
    "!pip install pdfplumber tqdm pandas python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae74a92-4934-4825-ba23-03c72831a326",
   "metadata": {},
   "source": [
    "Now we import the libraries that are used to parse the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92665de-f64d-443a-b471-693055147de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a39f8-e948-4770-a785-dd0ed191fb7b",
   "metadata": {},
   "source": [
    "Here, we define the input folders, as well as the output file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d73d839-8f41-499c-bc1d-366413d6eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "DOCX_FOLDER = \"../data/docx/\"          # Folder for Docx Files from Nexis\n",
    "PDF_FOLDER = \"../data/pdf/\"            # Folder for Pdf Files from Wiso\n",
    "OUTPUT_FILE = \"../data/csv/articles_raw.csv\" # Save extracted article data in a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffdb54-7056-46ed-96df-21a11a48eb4f",
   "metadata": {},
   "source": [
    "We declare two lists were each the docx and pdf articles are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8014cc11-e5c8-4ac8-a5cd-14c65fb10683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GLOBAL VARIABLES ---\n",
    "articles_docx = []\n",
    "articles_pdf = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925357c-b09f-49c8-b9ed-ae2196fb6fa0",
   "metadata": {},
   "source": [
    "This first helper function parses a single docx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f4f363-8681-4612-9c18-fcbb0a335ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTION 1 ---\n",
    "def docx_parse_file(path):\n",
    "    \"\"\"Parses a single Nexis DOCX file using 'Ende des Dokuments' as a splitter.\"\"\"\n",
    "    doc = Document(path)\n",
    "    local_articles = []\n",
    "    current_lines = []\n",
    "    splitter = \"Ende des Dokuments\"\n",
    "\n",
    "    # Regulae expression to find dates in German format (e.g., 15. Januar 2025)\n",
    "    date_pattern = re.compile(r'\\d{1,2}\\.?\\s+(?:Januar|Februar|März|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)\\s+\\d{4}', re.IGNORECASE)\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        txt = para.text.strip()\n",
    "        if not txt: continue \n",
    "\n",
    "        # Check if the end of an article block is reached\n",
    "        if splitter in txt:\n",
    "            if current_lines:\n",
    "                data = docx_extract_block(current_lines, date_pattern)\n",
    "                if data: local_articles.append(data)\n",
    "            current_lines = []\n",
    "        else:\n",
    "            current_lines.append(txt)\n",
    "\n",
    "    # Process the last block\n",
    "    if current_lines:\n",
    "        data = docx_extract_block(current_lines, date_pattern)\n",
    "        if data: local_articles.append(data)\n",
    "\n",
    "    return local_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6895d3-7cff-43aa-b34b-48bce53a1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTION 2 ---\n",
    "def docx_extract_block(lines, date_regex):\n",
    "    \"\"\"Extracts the metadata and text from a Nexis article block.\"\"\"\n",
    "\n",
    "    # Find the Copyright line as an anchor\n",
    "    copyright_index = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.lower().startswith(\"copyright\"):\n",
    "            copyright_index = i\n",
    "            break     \n",
    "    if copyright_index == -1: return None\n",
    "\n",
    "    # Search backwards from the copyright anchor to find the date\n",
    "    date_index = -1\n",
    "    date_str = \"\"\n",
    "    start_search = max(0, copyright_index - 10)\n",
    "    for i in range(copyright_index - 1, start_search, -1):\n",
    "        if date_regex.search(lines[i]):\n",
    "            date_index = i\n",
    "            date_str = lines[i]\n",
    "            break\n",
    "            \n",
    "    if date_index == -1:\n",
    "        date_index = copyright_index - 1\n",
    "        date_str = lines[date_index]\n",
    "\n",
    "    # Identify the source which is usually the line above the date\n",
    "    source = lines[date_index - 1] if date_index > 0 else \"Unbekannt\"\n",
    "\n",
    "    # Identify the title which is everything before the source\n",
    "    if date_index > 1:\n",
    "        raw_title_lines = lines[:date_index - 1]\n",
    "        title = \" \".join(raw_title_lines)\n",
    "    else:\n",
    "        title = \"Unknown\"\n",
    "\n",
    "    # Extract the body text (starts after 'Body' keyword or after Copyright)\n",
    "    body_index = -1\n",
    "    for i in range(copyright_index, len(lines)):\n",
    "        if lines[i] == \"Body\":\n",
    "            body_index = i\n",
    "            break\n",
    "            \n",
    "    text_start = body_index + 1 if body_index != -1 else copyright_index + 1\n",
    "\n",
    "    # Clean the text by removing the metadata lines\n",
    "    clean_text = []\n",
    "    for line in lines[text_start:]:\n",
    "        if line.startswith((\"Load-Date:\", \"Link to PDF\", \"Graphic\")): continue\n",
    "        if any(x in line for x in [\"Section:\", \"Length:\", \"Byline:\", \"Highlight:\"]): continue\n",
    "        clean_text.append(line)\n",
    "\n",
    "    # We then return the extracted date, source, title, and the cleaned text\n",
    "    return {\n",
    "        \"Date_Raw\": date_str,\n",
    "        \"Source\": source,\n",
    "        \"Title\": title,\n",
    "        \"Text\": \" \".join(clean_text)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cef846a-ad74-46ad-b229-80116efe17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTION 3 ---\n",
    "def pdf_parse_file(path):\n",
    "    \"\"\"Parses PDFs from Wiso\"\"\"\n",
    "    full_text_lines = []\n",
    "\n",
    "    # Extract the text from the pdf using the pdfplumber library\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        \n",
    "        # The first page is the table of contents so it can be skipped\n",
    "        start_page = 1 if len(pdf.pages) > 1 else 0\n",
    "        \n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            if i < start_page: continue \n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                lines = text.split('\\n')\n",
    "                # Filter out headers and footers\n",
    "                clean_lines = [l for l in lines if not l.startswith(\"Dokumente\") and not re.search(r'Seite \\d+ von \\d+', l)]\n",
    "                full_text_lines.extend(clean_lines)\n",
    "\n",
    "    local_articles = []\n",
    "    current_buffer = []\n",
    "    # Regex for dates (with format xx.xx.xx or xx.xx.xxxx)\n",
    "    date_regex = re.compile(r'(\\d{2}\\.\\d{2}\\.\\d{2,4})')\n",
    "\n",
    "    # Iterate through lines to build articles\n",
    "    for line in full_text_lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Check if a line indicates the end of an article, which is when it ends with Quelle (source)\n",
    "        if stripped_line.startswith(\"Quelle:\") or stripped_line.startswith(\"Quelle :\"):\n",
    "            if current_buffer:\n",
    "                # Extract metadata from the footer line\n",
    "                raw_meta = stripped_line\n",
    "                date_match = date_regex.search(raw_meta)\n",
    "                date_str = date_match.group(1) if date_match else None\n",
    "\n",
    "                # Clean up source name\n",
    "                source_clean = raw_meta.replace(\"Quelle:\", \"\").replace(\"Quelle :\", \"\").strip()\n",
    "                if date_match:\n",
    "                    source_clean = source_clean.split(date_str)[0].strip()\n",
    "                    source_clean = re.split(r',|Nr\\.', source_clean)[0].strip()\n",
    "\n",
    "                # We add a WISO tag so we can identify the extracted Wiso articles later\n",
    "                if \"(WISO)\" not in source_clean:\n",
    "                    source_clean = f\"{source_clean} (WISO)\"\n",
    "\n",
    "                # Now the title and text will be extracted\n",
    "                title = \"Unbekannt\"\n",
    "                body_text = \"\"\n",
    "\n",
    "                # Remove empty lines at the start\n",
    "                while current_buffer and not current_buffer[0].strip():\n",
    "                    current_buffer.pop(0)\n",
    "\n",
    "                # Set title as the first line and text as the rest\n",
    "                if current_buffer:\n",
    "                    title = current_buffer[0]\n",
    "                    body_text = \" \".join(current_buffer[1:])\n",
    "\n",
    "                # The extracted date, source, title, and the cleaned text are returned\n",
    "                local_articles.append({\n",
    "                    \"Date_Raw\": date_str,\n",
    "                    \"Source\": source_clean,\n",
    "                    \"Title\": title,\n",
    "                    \"Text\": body_text\n",
    "                })\n",
    "\n",
    "                # Reset the buffer for the next article\n",
    "                current_buffer = []\n",
    "\n",
    "        # Skip the metadata block lines that follow the source\n",
    "        elif any(marker in stripped_line for marker in [\"Ressort:\", \"Dokumentnummer:\", \"Dauerhafte Adresse\", \"Alle Rechte vorbehalten\", \"GENIOS\"]):\n",
    "            continue\n",
    "        else:\n",
    "            # Collect text line \n",
    "            current_buffer.append(stripped_line)\n",
    "\n",
    "    return local_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f0409-c935-4e9d-ad4c-199f3e3db6d3",
   "metadata": {},
   "source": [
    "This cell reads all .docx files from the docx input folder. Running this cell takes a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a722d2-a5ac-4dda-a2ce-be795c84e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 588 DOCX Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing DOCX: 100%|██████████| 588/588 [06:42<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in of DOCX complete. 70669 articles stored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PARSE DOCX FILES (TAKES 5-10 MINUTES) ---\n",
    "\n",
    "if os.path.exists(DOCX_FOLDER):\n",
    "    docx_files = [f for f in os.listdir(DOCX_FOLDER) if f.endswith(\".docx\") or f.endswith(\".DOCX\")]\n",
    "    print(f\"Found {len(docx_files)} DOCX Files.\")\n",
    "    \n",
    "    # Start with empty list\n",
    "    articles_docx = []\n",
    "\n",
    "    # Use tqdm for pretty progress bars\n",
    "    for filename in tqdm(docx_files, desc=\"Parsing DOCX\"):\n",
    "        path = os.path.join(DOCX_FOLDER, filename)\n",
    "        try:\n",
    "            extracted = docx_parse_file(path)\n",
    "            articles_docx.extend(extracted)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            \n",
    "    print(f\"Read in of DOCX complete. {len(articles_docx)} articles stored.\")\n",
    "else:\n",
    "    print(f\"Folder {DOCX_FOLDER} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da087cec-c54a-4180-b57a-7a4d19fb5ef4",
   "metadata": {},
   "source": [
    "Here, all pdf files are being read from the pdf input folder. Running this cell can take a long time, depending on the machine it might be 30-45 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a7d6a1-e7cd-4365-82a6-9a0d58be6a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 PDF Files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing PDF: 100%|██████████| 117/117 [40:43<00:00, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in of PDF complete. 6424 articles stored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PARSE PDF FILES (TAKES AROUND 30-45 MINUTES) ---\n",
    "\n",
    "if os.path.exists(PDF_FOLDER):\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith(\".pdf\")]\n",
    "    print(f\"Found {len(pdf_files)} PDF Files.\")\n",
    "    \n",
    "    # Start with empty list\n",
    "    articles_pdf = []\n",
    "\n",
    "    # Use tqdm for pretty progress bars\n",
    "    for filename in tqdm(pdf_files, desc=\"Parsing PDF\"):\n",
    "        path = os.path.join(PDF_FOLDER, filename)\n",
    "        try:\n",
    "            extracted = pdf_parse_file(path)\n",
    "            articles_pdf.extend(extracted)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            \n",
    "    print(f\"Read in of PDF complete. {len(articles_pdf)} articles stored.\")\n",
    "else:\n",
    "    print(f\"Folder {PDF_FOLDER} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1ee07-2105-4319-97f9-730eb8749fa8",
   "metadata": {},
   "source": [
    "All that is left is to combine the extracted data, filter junk, and create a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0245c3a7-458a-4946-9c9c-b08a7263f863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of raw extracted articles: 77093\n",
      "Filtering Nexis cover pages...\n",
      "Articles after cleaning: 76954\n",
      "Data successfully saved to: ../data/csv/articles_raw.csv\n",
      "\n",
      "Data Preview:\n",
      "               Source                                              Title\n",
      "0       Urner Zeitung           Das Lexikon für den gepflegten Smalltalk\n",
      "1   Groß-Gerauer Echo                                Maschine als Mensch\n",
      "2         Focus-Money  BUCHHALTUNGSPROGRAMME IM TEST; Software mit Gü...\n",
      "3  Allgemeine Zeitung                                Maschine als Mensch\n",
      "4    Berliner Zeitung  Wie in Hollywood; Das Computerspiel \"Total War...\n",
      "\n",
      "Top 5 Sources:\n",
      "Source\n",
      "dpa-AFX ProFeed                                                   4385\n",
      "Rheinische Post                                                   2136\n",
      "Neue Zürcher Zeitung (Internationale Ausgabe) & NZZ am Sonntag    1751\n",
      "SDA - Basisdienst Deutsch                                         1173\n",
      "Die Presse                                                        1112\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- JOIN, CLEAN AND SAVE DATA ---\n",
    "\n",
    "# Merge the two data lists\n",
    "all_data = articles_docx + articles_pdf\n",
    "print(f\"Total number of raw extracted articles: {len(all_data)}\")\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Remove the cover pages from Nexis, as they are not articles and we do not want to save them\n",
    "    print(\"Filtering Nexis cover pages...\")\n",
    "    mask_junk = df['Title'].str.contains(\"Job Number|Search Terms|Request ID\", case=False, na=False)\n",
    "    df_clean = df[~mask_junk]\n",
    "    \n",
    "    print(f\"Articles after cleaning: {len(df_clean)}\")\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    df_clean.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Data successfully saved to: {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Show a preview of the data\n",
    "    print(\"\\nData Preview:\")\n",
    "    print(df_clean[['Source', 'Title']].head())\n",
    "    print(\"\\nTop 5 Sources:\")\n",
    "    print(df_clean['Source'].value_counts().head())\n",
    "\n",
    "else:\n",
    "    print(\"No data found. Please make sure to run all cells beforehand.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
