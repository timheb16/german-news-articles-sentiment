{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d7f82d-ff33-413c-bd5d-74138d89a8ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Script 2 - Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89b3de-1faa-432e-a10b-1d5056cefaa8",
   "metadata": {},
   "source": [
    "_Script by Tim Hebestreit, thebestr@smail.uni-koeln.de_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b346453-b2e1-46af-9824-3d7813c9fb6a",
   "metadata": {},
   "source": [
    "In this notebook, the raw extracted articles are read in, cleaned, filtered, and saved with additional engineered features. The resulting corpus_final.csv is already present in the data/csv folder, but to replicate the process you can simply run all cells in this script. This process can take a short while, but should not be longer than 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70d13f4-2bd8-4174-ad1a-13d4efe57acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa166e9-6e13-46b2-903e-6efd5e4aeb5f",
   "metadata": {},
   "source": [
    "We define the input file (the raw articles exported using Script 1), and the name of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81fb955-ed5e-47c1-9ff0-2c8d16dcfd9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "INPUT_FILE = \"../data/csv/articles_raw.csv\"\n",
    "OUTPUT_FILE = \"../data/csv/corpus.csv\"\n",
    "\n",
    "# Enable tqdm for pandas operations\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f89cca-83fa-4e6a-a4fa-08adfa10e682",
   "metadata": {},
   "source": [
    "Here, we list the most common IT sources across different industries. This will be useful later to classify the articles as IT articles and general articles.\n",
    "\n",
    "We also define possible Press Release sources. Articles from these sources will be filtered out, as they are likely just corporate communication instead of journalistic articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dc658b-645f-4f4b-b1ed-a0945fcbe5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINE SOURCES ---\n",
    "\n",
    "# List possible Tech sources to catch IT articles\n",
    "# Allows distinction between general public media and specialized discourse\n",
    "\n",
    "# Disclaimer: As no comprehensive list was found online, these sources were originally generated with AI\n",
    "# and then validated based on which of these sources were actually present in the dataset\n",
    "IT_SOURCES = [\n",
    "    r'\\(wiso\\)', # Catch-all for WISO PDF exports\n",
    "    'autoflotte',\n",
    "    'autohaus',\n",
    "    'automobil',\n",
    "    'computerbild',\n",
    "    'computerwoche',\n",
    "    'egovernment',\n",
    "    'elektronik',\n",
    "    'elektrotechnik',\n",
    "    'energie & management',\n",
    "    'entsorga',\n",
    "    'internet world',\n",
    "    'it business',\n",
    "    'ix',\n",
    "    'kfz betrieb',\n",
    "    'konstruktion',\n",
    "    'labor praxis',\n",
    "    'logistik',\n",
    "    'logistra',\n",
    "    'macwelt',\n",
    "    'maschinenmarkt',\n",
    "    'pcwelt',\n",
    "    'process',\n",
    "    'telecom handel',\n",
    "    'transport',\n",
    "    'vdi',\n",
    "    'verkehrsrundschau',\n",
    "]\n",
    "\n",
    "# List is empty because preliminary checks showed no PR sources in the raw data\n",
    "# Kept as variable for consistency and potential future extendability\n",
    "PR_SOURCES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66d5d7-7bb1-4f89-84b5-e5a0f41c0651",
   "metadata": {},
   "source": [
    "Now the raw CSV file generated by Script 1 is loaded and its shape and number of missing values per column are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107eee0c-e907-4a8f-8fa2-9e5666209e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded. Initial shape: (76954, 4)\n",
      "\n",
      "Missing values per column:\n",
      "Date_Raw    3808\n",
      "Source         0\n",
      "Title          0\n",
      "Text         132\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD AND INSPECT RAW DATA ---\n",
    "\n",
    "if os.path.exists(INPUT_FILE):\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Raw data loaded. Initial shape: {df.shape}\")\n",
    "    \n",
    "    # Basic data integrity check\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "else:\n",
    "    print(f\"Input file not found: {INPUT_FILE}\")\n",
    "    print(\"Please run Script 1 first to generate the raw data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701834d-08be-4dad-832f-e72f0cd15fe1",
   "metadata": {},
   "source": [
    "Basic cleaning is performed in the next cell by removing rows without text or a title, as well as duplicate articles. Also, excessive whitespace is filtered from the text body of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2b9abd-1383-497d-ba7f-98fcd09f6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 12946 duplicates/empty rows.\n",
      "Remaining articles: 64008\n"
     ]
    }
   ],
   "source": [
    "# --- BASIC CLEANING ---\n",
    "\n",
    "initial_count = len(df)\n",
    "\n",
    "# Remove rows with missing text or title\n",
    "df = df.dropna(subset=['Text', 'Title'])\n",
    "\n",
    "# Text Cleanup\n",
    "# Remove excessive whitespace (newlines and tabs) to save space and make text cleaner\n",
    "# This is done before removing duplicates to catch duplicate articles differing only by whitespaces\n",
    "df['Text'] = df['Text'].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Remove Duplicates\n",
    "# Nexis can export duplicates if search batches overlap, and Nexis and Wiso articles can overlap\n",
    "# We assume that if title and text are identical, then it is the same article\n",
    "df = df.drop_duplicates(subset=['Title', 'Text'])\n",
    "\n",
    "cleaned_count = len(df)\n",
    "print(f\"Removed {initial_count - cleaned_count} duplicates/empty rows.\")\n",
    "print(f\"Remaining articles: {cleaned_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7226c-9f5d-4d1f-be73-33012ac926fe",
   "metadata": {},
   "source": [
    "Next, we wish to parse the date string to an actual date object. For that, we first define a helper function that parses a date string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0036dced-fe6d-4971-a2a6-4f87ddb0bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTION ---\n",
    "\n",
    "def parse_date(raw_date):\n",
    "    \"\"\"Parses date strings in German into datetime objects.\"\"\"\n",
    "    if not isinstance(raw_date, str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Clean up the date string (remove 'Date:', 'Datum:', extra spaces)\n",
    "    raw_date = raw_date.strip()\n",
    "    \n",
    "    # First look for standard German/English date string (DD. Month YYYY) with pattern:\n",
    "    # 1-2 digits\n",
    "    # optional dot\n",
    "    # space\n",
    "    # letters (for the month)\n",
    "    # optional dot (for abbreviations)\n",
    "    # space\n",
    "    # 4 digits (Year)\n",
    "    match = re.search(r'(\\d{1,2})\\.?\\s+([a-zA-ZäöüÄÖÜ]+)\\.?\\s+(\\d{4})', raw_date)\n",
    "    if match:\n",
    "        day, month, year = match.groups()\n",
    "        \n",
    "        # Translate the month if it is in German\n",
    "        # Check bothfull name and abbreviation\n",
    "        if month in german_months:\n",
    "            month = german_months[month]\n",
    "\n",
    "        # Normalize year (e.g. '20' -> '2020')\n",
    "        if len(year) == 2:\n",
    "            year = \"20\" + year\n",
    "            \n",
    "        \n",
    "        # Tries to return a clean string, e.g. 15 October 2025, for pandas to turn into datetime object\n",
    "        try:\n",
    "            return pd.to_datetime(f\"{day} {month} {year}\", errors='coerce')\n",
    "        except:\n",
    "            pass # Fall through to next strategy\n",
    "\n",
    "    # If the first regex did not match look for numeric dates (DD.MM.YYYY or DD.MM.YY)\n",
    "    match_numeric = re.search(r'(\\d{1,2})\\.(\\d{1,2})\\.(\\d{2,4})', raw_date)\n",
    "    if match_numeric:\n",
    "        # Tries to return datetime object\n",
    "        try:\n",
    "            return pd.to_datetime(match_numeric.group(0), dayfirst=True, errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    # If both previous checks did not match, catch date strings in English format (e.g. January 11, 2019) with pattern:\n",
    "    # Month name\n",
    "    # Space\n",
    "    # 1-2 digits\n",
    "    # Comma\n",
    "    # Space\n",
    "    # 2-4 digits\n",
    "    match_en = re.search(r'([a-zA-Z]+)\\s+(\\d{1,2}),\\s+(\\d{2,4})', raw_date)\n",
    "    if match_en:\n",
    "        try:\n",
    "            m, d, y = match_en.groups()\n",
    "            return pd.to_datetime(f\"{d} {m} {y}\", errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pd.NaT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324a2f3-4c4b-46ab-88c2-e912476cfa52",
   "metadata": {},
   "source": [
    "Now the date is parsed using that function, and rows are dropped where no date could be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28728c4-fb1a-4866-896f-647eaac5d757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9061349aa89a468594468351237bc8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 8744 rows with unparseable dates.\n",
      "Dates parsed. Date range of the articles: 2015-11-01 to 2025-11-01\n",
      "Valid articles in timeframe: 55263\n"
     ]
    }
   ],
   "source": [
    "# --- PARSE DATES ---\n",
    "\n",
    "# Dictionary to map German months\n",
    "german_months = {\n",
    "    'Januar': 'January', 'Februar': 'February', 'März': 'March', 'Mai': 'May',\n",
    "    'Juni': 'June', 'Juli': 'July', 'Oktober': 'October', 'Dezember': 'December',\n",
    "    # Also want to catch abbreviations\n",
    "    'Jan': 'January', 'Feb': 'February', 'Mrz': 'March', 'Apr': 'April',\n",
    "    'Jun': 'June', 'Jul': 'July', 'Aug': 'August', 'Sep': 'September', \n",
    "    'Okt': 'October', 'Nov': 'November', 'Dez': 'December'\n",
    "}\n",
    "\n",
    "# Apply parsing using the helper function\n",
    "df['Date'] = df['Date_Raw'].progress_apply(parse_date)\n",
    "\n",
    "# Drop rows where date could not be parsed\n",
    "before_date_drop = len(df)\n",
    "df = df.dropna(subset=['Date'])\n",
    "print(f\"Dropped {before_date_drop - len(df)} rows with unparseable dates.\")\n",
    "\n",
    "# Extract date only (remove time of day)\n",
    "df['Date'] = df['Date'].dt.date\n",
    "\n",
    "# Time filternsure we only keep articles within the relevant study period.\n",
    "# This removes parsing errors (e.g., finding a birthdate \"1990\" in text)\n",
    "START_DATE = pd.to_datetime(\"2015-11-01\").date()\n",
    "END_DATE = pd.to_datetime(\"2025-11-01\").date()\n",
    "df = df[(df['Date'] >= START_DATE) & (df['Date'] <= END_DATE)]\n",
    "\n",
    "# Finally, drop the now unused Date_Raw column\n",
    "df.drop(\"Date_Raw\", axis='columns', inplace=True)\n",
    "\n",
    "print(f\"Dates parsed. Date range of the articles: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Valid articles in timeframe: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0fd376-5ece-46db-b85b-5f5f2fedf21e",
   "metadata": {},
   "source": [
    "Several artificial features used for the final filtering and the analysis are added to the DataFrame:\n",
    "- *Word_Count*: This is useful to filter out snippets, image captions, or other short messages that are not articles and have not enough semantic substance to analyse sentiment.\n",
    "- *Is_IT_Source*: A flag that distinguishes IT articles from public discourse.\n",
    "- *Is_PR*: To filter out articles from Press Release agencies, as we only want to keep journalistic articles and not corporate communication. CURRENTLY INACTIVE, MIGHT BE READDED LATER\n",
    "- *Year* and *Month*: Temporal features useful for aggregation and plots later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15381d19-1ff2-4068-84b8-913d739c5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FEATURE ENGINEERING ---\n",
    "\n",
    "# Word Count\n",
    "df['Word_Count'] = df['Text'].str.split().str.len()\n",
    "\n",
    "# Is IT Source (true if the source is in the IT source list)\n",
    "pattern_it = '|'.join(IT_SOURCES)\n",
    "df['Is_IT_Source'] = df['Source'].astype(str).str.contains(pattern_it, case=False, regex=True)\n",
    "\n",
    "# Press Release (true if the source is a PR agency) MIGHT BE ADDED AGAIN LATER\n",
    "pattern_pr = '|'.join(PR_SOURCES)\n",
    "df['Is_PR'] = df['Source'].astype(str).str.contains(pattern_pr, case=False, regex=True)\n",
    "\n",
    "# Year and Month\n",
    "df['Year'] = pd.to_datetime(df['Date']).dt.year\n",
    "df['Month'] = pd.to_datetime(df['Date']).dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad154a-cc4a-4cfd-a05e-5de9a0ec09f8",
   "metadata": {},
   "source": [
    "Remove all press releases using the Is_PR flag, and remove short articles with less than 40 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f4eee9-a220-4b11-a7d2-8113f1726426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleanup: 55263 -> 54133 articles.\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL FILTERING ---\n",
    "\n",
    "original_len = len(df)\n",
    "\n",
    "# Remove Press Releases MIGHT BE READDED LATER\n",
    "#df = df[df['Is_PR'] == False]\n",
    "\n",
    "# Remove short snippets that are likely image captions, snippets, errors, or very short messages\n",
    "MIN_WORD_COUNT = 40\n",
    "df = df[df['Word_Count'] >= MIN_WORD_COUNT]\n",
    "\n",
    "print(f\"Final cleanup: {original_len} -> {len(df)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500adea5-c3ef-45eb-b8e2-8c789bf8e4af",
   "metadata": {},
   "source": [
    "The final corpus is now saved to a csv file at *data/csv/corpus_final.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8043ac38-9cf2-45f5-8995-9ac57f8f6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL CORPUS CREATED: ../data/csv/corpus.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE CORPUS ---\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"FINAL CORPUS CREATED: {OUTPUT_FILE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a43942-18dd-43dc-8001-4111a2a865c3",
   "metadata": {},
   "source": [
    "Here is an overview of some basic statistics of the saved DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0738f616-2b0e-494f-b853-bbd8af7302cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Articles: 54133\n",
      "----------------------------------------\n",
      "Distribution by Source Type:\n",
      "Is_IT_Source\n",
      "General Articles    47087\n",
      "IT Articles          7046\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Timeframe:\n",
      "Start: 2015-11-01\n",
      "End:   2025-11-01\n",
      "----------------------------------------\n",
      "Average Word Count: 714\n"
     ]
    }
   ],
   "source": [
    "# --- STATISTICS ---\n",
    "\n",
    "print(f\"Total Articles: {len(df)}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Distribution by Source Type:\")\n",
    "print(df['Is_IT_Source'].value_counts().rename({True: 'IT Articles', False: 'General Articles'}))\n",
    "print(\"-\" * 40)\n",
    "print(\"Timeframe:\")\n",
    "print(f\"Start: {df['Date'].min()}\")\n",
    "print(f\"End:   {df['Date'].max()}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Average Word Count:\", int(df['Word_Count'].mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
